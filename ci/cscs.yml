include:
  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'

stages:
  - build
  - testSF
  - ensureData
  - runTrain1
  - runTrain2
  - runMatch1

variables:
  PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/nettest:$CI_COMMIT_SHA
  CSCS_ADDITIONAL_MOUNTS: '["/iopsstor/scratch/cscs/vjoost/nettest/data:/workspace/data", "/capstor/scratch/cscs/vjoost/nettest/scratch/:/workspace/scratch/"]'

build_job:
  stage: build
  extends: .container-builder-cscs-gh200
  variables:
    DOCKERFILE: ci/docker/Dockerfile.build

# ensure data is in place
ensureData:
  timeout: 24h
  stage: ensureData
  extends: .container-runner-clariden-gh200
  image: $PERSIST_IMAGE_NAME
  script:
    - /workspace/nettest/do_ensure_data.sh /workspace/data linrock test60
    - /workspace/nettest/do_ensure_data.sh /workspace/data linrock test77
    - /workspace/nettest/do_ensure_data.sh /workspace/data linrock test78
    - /workspace/nettest/do_ensure_data.sh /workspace/data linrock test79
    - /workspace/nettest/do_ensure_data.sh /workspace/data linrock test80-2022
    - /workspace/nettest/do_ensure_data.sh /workspace/data linrock test80-2023
    - /workspace/nettest/do_ensure_data.sh /workspace/data linrock test80-2024
    - /workspace/nettest/do_ensure_data.sh /workspace/data linrock dual-nnue
    - /workspace/nettest/do_ensure_data.sh /workspace/data official-stockfish master-binpacks
    - /workspace/nettest/do_ensure_data.sh /workspace/data official-stockfish master-smallnet-binpacks
  variables:
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    SLURM_TIMELIMIT: '12:00:00'

# A first step of training. Probably doing multiple step can be organized a bit differently, possibly dynamically ? TODO: --ft_optimize (too slow for testing)
runTrain1:
  timeout: 24h
  stage: runTrain1
  extends: .container-runner-clariden-gh200
  image: $PERSIST_IMAGE_NAME
  script:
    - cd /workspace/nnue-pytorch
    - python train.py /workspace/data/from_classical_04_pv-2_diff-100_nodes-5000.binpack --gpus="0," --threads 16 --num-workers 16 --max_epochs 2 --network-save-period 2 --enable_progress_bar false  --batch-size 16384 --random-fen-skipping 3 --features=HalfKAv2_hm^ --default_root_dir /workspace/scratch/$CI_COMMIT_SHA/training/runs/run_1
    - export OMP_NUM_THREADS=8
    - python serialize.py /workspace/scratch/$CI_COMMIT_SHA/training/runs/run_1/lightning_logs/version_0/checkpoints/last.ckpt /workspace/scratch/$CI_COMMIT_SHA/training/runs/run_1/lightning_logs/version_0/checkpoints/last.nnue --features=HalfKAv2_hm^ --ft_compression=leb128 --ft_optimize_count=1000000 --ft_optimize_data=/workspace/data/official-stockfish/master-binpacks/test80-2022-08-aug-16tb7p.v6-dd.min.binpack
  variables:
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    SLURM_TIMELIMIT: '12:00:00'

# A second step of training. Not it resumes and continues a previous trainig. Probably doing multiple step can be organized a bit differently, possibly dynamically ? TODO: --ft_optimize (too slow for testing)
runTrain2:
  timeout: 24h
  stage: runTrain2
  extends: .container-runner-clariden-gh200
  image: $PERSIST_IMAGE_NAME
  script:
    - cd /workspace/nnue-pytorch
    - python train.py /workspace/data/from_classical_04_pv-2_diff-100_nodes-5000.binpack --gpus="0," --threads 16 --num-workers 16 --max_epochs 4 --network-save-period 2 --enable_progress_bar false --batch-size 16384 --random-fen-skipping 3 --features=HalfKAv2_hm^ --default_root_dir /workspace/scratch/$CI_COMMIT_SHA/training/runs/run_2 --resume_from_checkpoint /workspace/scratch/$CI_COMMIT_SHA/training/runs/run_1/lightning_logs/version_0/checkpoints/last.ckpt
    - export OMP_NUM_THREADS=8
    - python serialize.py /workspace/scratch/$CI_COMMIT_SHA/training/runs/run_2/lightning_logs/version_0/checkpoints/last.ckpt /workspace/scratch/$CI_COMMIT_SHA/training/runs/run_2/lightning_logs/version_0/checkpoints/last.nnue --features=HalfKAv2_hm^ --ft_compression=leb128 --ft_optimize_count=1000000 --ft_optimize_data=/workspace/data/official-stockfish/master-binpacks/test80-2022-08-aug-16tb7p.v6-dd.min.binpack
  variables:
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    SLURM_TIMELIMIT: '12:00:00'

# A final test with a match, need to see if we can run with 280 concurrency, probably not, rather 1 per socket?
# Should probably become a sprt.
runMatch1:
  timeout: 24h
  stage: runMatch1
  extends: .container-runner-clariden-gh200
  image: $PERSIST_IMAGE_NAME
  script:
    - mkdir -p /workspace/scratch/$CI_COMMIT_SHA/match
    - cd /workspace/scratch/$CI_COMMIT_SHA/match
    - /workspace/fastchess/fastchess -rounds 600 -games 2 -repeat -srand 42  -concurrency 280 -openings file=/workspace/data/UHO_Lichess_4852_v1.epd format=epd order=random -engine name=master cmd=/workspace/Stockfish/src/stockfish -engine name=runTrain1 cmd=/workspace/Stockfish/src/stockfish option.EvalFile=/workspace/scratch/$CI_COMMIT_SHA/training/runs/run_1/lightning_logs/version_0/checkpoints/last.nnue -engine name=runTrain2 cmd=/workspace/Stockfish/src/stockfish option.EvalFile=/workspace/scratch/$CI_COMMIT_SHA/training/runs/run_2/lightning_logs/version_0/checkpoints/last.nnue -each proto=uci option.Threads=1 option.Hash=16 tc=10+0.1 -ratinginterval 280 -report penta=true -pgnout file=match.pgn
  variables:
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    SLURM_TIMELIMIT: '12:00:00'
